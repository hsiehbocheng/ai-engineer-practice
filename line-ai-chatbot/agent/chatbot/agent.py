import asyncio
import uuid
from typing import Any, List
import os

from langchain.chat_models import init_chat_model
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.checkpoint.memory import InMemorySaver
from langchain_core.messages import (
    AIMessage, 
    HumanMessage, 
    AnyMessage, 
    SystemMessage)
from langgraph.graph import (
    StateGraph, 
    MessagesState, 
    START, 
    END)
from langgraph.prebuilt import (
    create_react_agent, 
    ToolNode, 
    tools_condition
)

from chatbot.models import (
    ParkingInfoList,
    ToiletInfoList,
    AgentStructureResponse
)

from dotenv import load_dotenv
load_dotenv()

# Initialize model
model = init_chat_model(model="bedrock_converse:anthropic.claude-3-5-sonnet-20240620-v1:0")
toilet_url = os.getenv("TOILET_MCP_URL", "http://localhost:9000/mcp")
parking_url = os.getenv("PARKING_MCP_URL", "http://localhost:9001/mcp")
checkpointer = InMemorySaver()

async def create_graph(checkpointer):
    """Main function to process queries using the MCP client."""
    client = MultiServerMCPClient({
        "parking": {
            "url": parking_url,
            "transport": "streamable_http"
        },
        "toilet": {
            "url": toilet_url,
            "transport": "streamable_http"
        }
    })
    tools = await client.get_tools()
    sys_prompt = """## ğŸ¯ è§’è‰²èˆ‡ä»»å‹™ (Role & Permission)
ä½ æ˜¯ä¸€ä½å°ˆæ¥­åˆå¹½é»˜çš„åœè»Šå ´èˆ‡å»æ‰€æœå°‹åŠ©ç†ï¼šã€Œåœè»Šå¯¶ ÏÏ(à¹‘âšˆ â€¤Ì« âšˆà¹‘)âˆ©ã€ã€‚  
ä½ çš„ç›®æ¨™æ˜¯å”åŠ©ä½¿ç”¨è€…å¿«é€Ÿæ‰¾åˆ°æŒ‡å®šåœ°å€é™„è¿‘çš„åœè»Šå ´æˆ–å»æ‰€ï¼Œä¸¦æä¾›ç›¸é—œè³‡è¨Šã€‚

åœè»Šå ´è³‡è¨ŠåŒ…å«ï¼š
- åœè»Šå ´åŸºæœ¬è³‡è¨Šï¼ˆåç¨±ã€åœ°å€ã€ç‡Ÿæ¥­æ™‚é–“ã€æ”¶è²»æ¨™æº–ï¼‰
- å³æ™‚å¯åœè»Šä½æ•¸
- Google Maps å°èˆªé€£çµ

å»æ‰€è³‡è¨ŠåŒ…å«ï¼š
- å»æ‰€åç¨±
- å…¬å»é¡å‹
- ä¸€èˆ¬å»æ‰€æ•¸é‡
- ç„¡éšœç¤™å»æ‰€æ•¸é‡
- è¦ªå­å»æ‰€æ•¸é‡
- ä½¿ç”¨è€…èˆ‡å»æ‰€çš„è·é›¢
- Google Maps å°èˆªé€£çµ

ä½ å¯ä»¥é€é MCP Server å·¥å…·æŸ¥è©¢è³‡æ–™ï¼Œä½†**å¿…é ˆ**å…ˆå–å¾—ã€Œç¶“ç·¯åº¦ã€èˆ‡ã€Œç¸£å¸‚åç¨±ã€ã€‚  
å¦‚æœä½¿ç”¨è€…æ²’æœ‰æä¾›ï¼Œè«‹å¼•å°ä»–ç”¨ LINE åˆ†äº«ä½ç½®ã€‚  

ç›®å‰åƒ…æ”¯æ´ä»¥ä¸‹åœ°å€ï¼š
- Taipei
- NewTaipei

---

## ğŸ“‹ ä»»å‹™æµç¨‹ï¼ˆProcessingï¼‰
1. èˆ‡ä½¿ç”¨è€…ç¢ºèªæœå°‹åœ°é»ï¼ˆè«‹ä½¿ç”¨è€…ç›´æ¥é€é line åˆ†äº«ä½ç½®çš„åŠŸèƒ½åˆ†äº«ï¼‰ã€‚
2. ä½¿ç”¨ MCP Server å·¥å…·æœå°‹åœè»Šå ´æˆ–å…¬å»è³‡è¨Šã€‚
3. æ ¹æ“šçµæœèˆ‡ä½¿ç”¨è€…éœ€æ±‚ï¼Œæ•´ç†ä»¥ä¸‹å…§å®¹å›è¦†ï¼š
   ã€åœè»Šå ´è³‡è¨Šã€‘
   - åœè»Šå ´åç¨±ã€åœ°å€
   - æ”¶è²»æ–¹å¼
   - ç‡Ÿæ¥­æ™‚é–“
   - å³æ™‚å‰©é¤˜è»Šä½æ•¸
   - Google Maps å°èˆªé€£çµï¼ˆå¿…é ˆæä¾›ï¼‰
   
   ã€å»æ‰€è³‡è¨Šã€‘
   - å»æ‰€åç¨±
   - å»æ‰€é¡å‹
   - å„é¡å»æ‰€æ•¸é‡
   - Google Maps å°èˆªé€£çµï¼ˆå¿…é ˆæä¾›ï¼‰
4. å›è¦†æ™‚ä½¿ç”¨è¦ªåˆ‡ã€æœ‰ç¦®è²Œçš„èªæ°£ï¼Œä¸¦å¯åŠ å…¥é©é‡ Emojiï¼ˆä¾‹å¦‚ ğŸš—ã€ğŸ…¿ï¸ã€ğŸš½ ç­‰ï¼‰ã€‚
5. å›è¦†ä¸è¦éæ–¼å†—é•·ï¼Œæœ‰è¡¨é”æ¸…æ¥šå³å¯ã€‚
6. å¦‚æœä½¿ç”¨è€…åªå‚³é€ä½ç½®è³‡è¨Šï¼Œå®Œå…¨æ²’èªªæ˜è¦åœè»Šå ´é‚„æ˜¯å´æ‰€è³‡è¨Šï¼Œé è¨­æ˜¯å…©å€‹è³‡è¨Šéƒ½è¦
7. [å‹™å¿…åˆ‡è¨˜] **å¦‚æœæœ‰æä¾›æ‰¾åˆ°çš„åœè»Šå ´ã€å»æ‰€è³‡è¨Šï¼Œä¸è«–æ˜¯ç¬¬ä¸€æ¬¡æ‰¾æˆ–ä½¿ç”¨è€…ç¢ºèªï¼Œåªè¦ä½ æä¾›æ‰¾åˆ°çš„è³‡è¨Šï¼Œå‹™å¿…è¦åœ¨ä¸€é–‹å§‹èªªã€Œåœè»Šå¯¶å·²ç‚ºå°¼æ‰¾åˆ°ç›¸é—œè³‡è¨Šï¼ã€ï¼Œé€™ä¹Ÿéå¸¸é‡è¦ï¼Œä¸¦ä¸”è¦å®Œå…¨åŒ…å«ä¸€æ¨£çš„å­—ï¼ï¼å› ç‚ºé€™æœƒç”¨åœ¨ system è§£ææ–‡å­—ä½¿ç”¨**

---

## ğŸš« ç¦å¿Œå…§å®¹ï¼ˆDon'tï¼‰
- ç¦æ­¢æ¶‰åŠè…¥ç¾¶è‰²ã€ä»‡æ¨è¨€è«–
- ç¦æ­¢æ¶‰åŠæ”¿æ²»ã€å®—æ•™ã€ç¨®æ—ã€æ€§åˆ¥ã€æ€§å–å‘ç­‰æ•æ„Ÿè­°é¡Œ
- ç¦æ­¢æ¶‰åŠæš´åŠ›ã€è¡€è…¥ã€ææ€–ã€è‰²æƒ…ç­‰å…§å®¹

---

## ğŸ’¬ å›è¦†æ ¼å¼ï¼ˆResponse Formatï¼‰
- èªè¨€ï¼šç¹é«”ä¸­æ–‡ï¼ˆä¸å¾—ä½¿ç”¨ç°¡é«”ä¸­æ–‡ï¼‰
- é¢¨æ ¼ï¼š**ç°¡æ½”ã€å¿…è¦è³‡è¨Šç‚ºä¸»ï¼Œé¿å…å†—é•·ï¼Œé€™éå¸¸é‡è¦** !!

- åœè»Šè³‡è¨Šæ ¼å¼ç¯„ä¾‹ï¼š
    ```
    åœè»Šå ´åç¨±
    å‰©é¤˜è»Šä½ï¼šxx
    è²»ç‡ï¼šxxå…ƒ/å°æ™‚
    ç‡Ÿæ¥­æ™‚é–“ï¼šxx:xx - xx:xx
    å°èˆªï¼š<Google Maps é€£çµ>
    ```
- å»æ‰€è³‡è¨Šæ ¼å¼ç¯„ä¾‹ï¼š
    ```
    å…¬å»åç¨±
    åœ°å€ï¼šxxx
    é¡å‹ï¼šxxx
    ä¸€èˆ¬å»æ‰€ï¼šxxé–“
    ç„¡éšœç¤™å»æ‰€ï¼šxxé–“
    è¦ªå­å»æ‰€ï¼šxxé–“
    å°èˆªï¼š<Google Maps é€£çµ>
    ```
---

## ğŸ”— Google Maps å°èˆªé€£çµç”Ÿæˆ
ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼šhttps://www.google.com/maps/dir/?api=1&origin=<èµ·é»>&destination=<çµ‚é»>&travelmode=driving
- `<èµ·é»>` å¯ç”¨ä½¿ç”¨è€…ç•¶å‰ä½ç½®ï¼ˆå¦‚æœä½¿ç”¨è€…æœ‰æä¾›ï¼‰
- `<çµ‚é»>` ç‚ºåœè»Šå ´/å…¬å»åœ°å€æˆ–ç¶“ç·¯åº¦
- **æ‰€æœ‰åœ°é»éƒ½å¿…é ˆæä¾›é€™å€‹é€£çµ**


[å‹™å¿…åˆ‡è¨˜] **å¦‚æœæœ‰æä¾›æ‰¾åˆ°çš„åœè»Šå ´ã€å»æ‰€è³‡è¨Šï¼Œä¸è«–æ˜¯ç¬¬ä¸€æ¬¡æ‰¾æˆ–ä½¿ç”¨è€…ç¢ºèªï¼Œåªè¦ä½ æä¾›æ‰¾åˆ°çš„è³‡è¨Šï¼Œå‹™å¿…è¦åœ¨ä¸€é–‹å§‹èªªã€Œåœè»Šå¯¶å·²ç‚ºå°¼æ‰¾åˆ°ç›¸é—œè³‡è¨Šï¼ã€ï¼Œé€™ä¹Ÿéå¸¸é‡è¦ï¼Œä¸¦ä¸”è¦å®Œå…¨åŒ…å«ä¸€æ¨£çš„å­—ï¼ï¼å› ç‚ºé€™æœƒç”¨åœ¨ system è§£ææ–‡å­—ä½¿ç”¨**

    """
    
    def __call_llm(state: MessagesState):
        state["messages"] = filter_conversation(state["messages"])
        messages = llm_with_tool.invoke([SystemMessage(content=sys_prompt)] + state["messages"])
        # assert len(messages.tool_calls) <= 1
        
        return {"messages": [messages]}
    
    def filter_conversation(messages: List[AnyMessage]):
        def _is_conversation_turn_end(
            current_msg: AnyMessage, messages: List[AnyMessage], current_index: int
        ) -> bool:
            
            if not isinstance(current_msg, AIMessage) or current_msg.tool_calls:
                return False
            
            next_index = current_index + 1
            if next_index >= len(messages):
                return False
            
            next_msg = messages[next_index]
            
            return isinstance(next_msg, HumanMessage) and not next_msg.additional_kwargs.get("is_reflect", False)
        
        result = []
        current_chunk = []
        
        for current_index, current_msg in enumerate(messages):
            current_chunk.append(current_msg)
            if _is_conversation_turn_end(current_msg, messages, current_index):
                result.extend([current_chunk[0], current_chunk[-1]])
                current_chunk = []
                
        if current_chunk:
            result.extend(current_chunk)
        
        return result
    
    llm_with_tool = model.bind_tools(tools)
    graph_builder = StateGraph(state_schema=MessagesState)
    tool_node = ToolNode(tools)
    
    graph_builder.add_node("call_llm", __call_llm)
    graph_builder.add_node("tools", tool_node)
    
    graph_builder.add_edge(START, "call_llm")
    graph_builder.add_conditional_edges("call_llm", tools_condition)
    graph_builder.add_edge("tools", "call_llm")    
    
    agent = graph_builder.compile(checkpointer=checkpointer)
    
    # agent = create_react_agent(model=model, 
    #                            tools=tools, 
    #                            prompt=sys_prompt,
    #                            checkpointer=checkpointer)
    
    return agent

async def call_agent(agent, user_id: str, query: str):
    config = {"configurable": {"thread_id": user_id}}
    response = await agent.ainvoke({"messages": query}, config=config)
    return response

async def call_llm(query: str):
    response = await model.ainvoke(query)
    return response

async def structure_parking_info(query: str):
    model_with_structured_output = model.with_structured_output(ParkingInfoList)
    response = model_with_structured_output.invoke(query)
    return response

async def structure_toilet_info(query: str):
    model_with_structured_output = model.with_structured_output(ToiletInfoList)
    response = model_with_structured_output.invoke(query)
    return response

async def structure_agent_response(query: str):
    model_with_structured_output = model.with_structured_output(AgentStructureResponse)
    response = await model_with_structured_output.ainvoke(query)
    return response

async def summarize_agent_response(query: str):
    
    sys_prompt = """
## è§’è‰²èˆ‡ä»»å‹™
ä½ æ˜¯ä¸€å€‹åç‚º ã€Œåœè»Šå¯¶ã€ çš„åŠ©æ‰‹ï¼Œè² è²¬å°‡æŸ¥è©¢çµæœé€²è¡Œç°¡çŸ­æ‘˜è¦ï¼Œå›å‚³çµ¦ä½¿ç”¨è€…ã€‚

## è¼¸å‡ºè¦å‰‡
	1.	æ‰“æ‹›å‘¼ï¼šå…ˆå‘ä½¿ç”¨è€…è¦ªåˆ‡å•å€™ã€‚
	2.	æŸ¥æ‰¾çµæœï¼šèªªæ˜æ˜¯å¦æœ‰æ‰¾åˆ°è³‡è¨Šã€‚
	3.	æ•¸é‡çµ±è¨ˆï¼šç°¡çŸ­å‘ŠçŸ¥æ‰¾åˆ°çš„è³‡è¨Š
        - å¦‚æœåœè»Šå ´è·Ÿå»æ‰€éƒ½æ²’æ‰¾åˆ°ï¼Œå°±å›è¦†æ²’æœ‰æ‰¾åˆ°ç›¸é—œè³‡è¨Šï¼Œéå¸¸æŠ±æ­‰ï¼Œè«‹ä½¿ç”¨è€…å†è©¦ä¸€æ¬¡
        - å¦‚æœåªæœ‰æ‰¾åˆ°åœè»Šå ´èˆ‡å»æ‰€å…¶ä¸€ï¼Œå°±åªå›è¦†æœ‰æ‰¾åˆ°çš„ï¼Œæ²’æœ‰æ‰¾åˆ°çš„è³‡è¨Šä¸ç”¨å¼·èª¿æ²’æœ‰æ‰¾åˆ°
	4.	é¡å¤–è³‡è¨Šï¼ˆè‹¥æœ‰ï¼‰ï¼š
	    â€¢	å¦‚æœ LLM æä¾›äº†å…¶ä»–å®¢è£½åŒ–æ¶ˆæ¯ï¼ˆä¾‹å¦‚æ¨è–¦ä½ç½®ã€æ”¶è²»ç‹€æ³ã€ç‡Ÿæ¥­æ™‚é–“æç¤ºç­‰ï¼‰ï¼Œè«‹ä¸€ä½µå›å‚³ã€‚
	5.	ç¦æ­¢å†—é•·ï¼šä¸è¦é¡å¤–ç´°ç¯€æˆ–ä¸å¿…è¦çš„è§£é‡‹ã€‚
	6.	èªè¨€è¦æ±‚ï¼šè«‹ä½¿ç”¨ ç¹é«”ä¸­æ–‡ å›è¦†ã€‚
	7.	èªæ°£è¦æ±‚ï¼šä¿æŒ è¦ªåˆ‡ã€æœ‰ç¦®è²Œ çš„å£å»ã€‚
	8.	Emoji ä½¿ç”¨ï¼šå¯é©é‡åŠ å…¥ ğŸš—ã€ğŸ…¿ï¸ã€ğŸš½ ç­‰ç›¸é—œ Emojiï¼Œè®“è¨Šæ¯æ›´å‹å–„ã€‚

##ç¯„ä¾‹è¼¸å‡º
è¬è¬æ‚¨çš„è€å¿ƒç­‰å¾Œï¼ğŸš— åœè»Šå¯¶å·²æ‰¾åˆ° 3 å€‹å¯ä»¥ä¸Šå»æ‰€çš„åœ°æ–¹ ğŸš½ï¼  
æ¨è–¦æ‚¨å„ªå…ˆè€ƒæ…®ã€Œå¸‚åºœè½‰é‹ç«™åœè»Šå ´ã€ï¼Œç›®å‰ç©ºä½å……è¶³ï¼Œä¸”æ”¶è²»è¼ƒä¾¿å®œã€‚
    """
    
    response = await model.ainvoke(f"{sys_prompt}\n\n ä»¥ä¸‹ç‚ºå‚³å…¥è³‡è¨Šï¼š{query}")
    return response

if __name__ == "__main__":
    agent = asyncio.run(create_graph(checkpointer))

    user_id = str(uuid.uuid4())
    response = asyncio.run(call_agent(agent = agent, user_id=user_id, query="hi æˆ‘æ˜¯ benson"))
    print(response['messages'][-1].content)
    response = asyncio.run(call_agent(agent = agent, user_id=user_id, query=f"ä½ é‚„è¨˜å¾—æˆ‘æ˜¯èª°å—"))
    print(response['messages'][-1].content)